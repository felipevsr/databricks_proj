{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5d88f59b-8e91-427b-883e-8ff736ea66ef",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "![IBGE LOGO](https://s3-sa-east-1.amazonaws.com/arrematearte-farm/cristinagoston/lot_photos/11920/e2b04faba30f14f269a12c88c1dffc839e637495_ml.jpg)\n",
    "\n",
    "\n",
    "\n",
    "# CAMADA BRONZE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "74f8bbe4-6465-49e0-9d6e-40c8c2b9047d",
     "showTitle": true,
     "title": "Imports"
    }
   },
   "outputs": [],
   "source": [
    "from delta.tables import *\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import col\n",
    "import pytz\n",
    "from pyspark.sql.functions import lit\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.functions import *\n",
    "from datetime import datetime\n",
    "\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cb9a66a9-c7f5-4199-af71-23ae10eea3e4",
     "showTitle": true,
     "title": "Variaveis"
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##----------------------------##\nData e Horário de execusão             ===> 20241020_100532 \n\nTabela bronze a ser criada             ===>  bronze.ibge_news\nCaminho da tabela Bronze a ser criada  ===> dbfs:/mnt/bronze/ \n\n##----------------------------##\nSistema de origem                      ===> dbfs:/mnt/raw_3/\nCaminho para gravação do histórico     ===> dbfs:/mnt/historic/bronze/\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  time_file = datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%Y%m%d_%H%M%S')\n",
    "  # DeltaTable.isDeltaTable(spark, bronze_delta_path)\n",
    "\n",
    "  #### Nome  e caminho onde será feita a escrita da da tabela Delta  ##########\n",
    "  db = 'bronze.'\n",
    "  table = 'ibge_news'\n",
    "  delta_table = f'{db}{table}'\n",
    "  bronze_delta_path = 'dbfs:/mnt/bronze/'\n",
    "\n",
    "  ### Sistema de origem ####\n",
    "  source_system_path = 'dbfs:/mnt/raw_3/'\n",
    "\n",
    "  ### caminho para gravação do histórico ###\n",
    "  historic_path = 'dbfs:/mnt/historic/bronze/'\n",
    "  \n",
    "  print(\"##----------------------------##\")\n",
    "  print(f\"Data e Horário de execusão             ===> {time_file} \\n\")\n",
    "  print(f\"Tabela bronze a ser criada             ===>  {delta_table}\")\n",
    "  print(f\"Caminho da tabela Bronze a ser criada  ===> {bronze_delta_path} \\n\")\n",
    "  print(\"##----------------------------##\")\n",
    "  print(f\"Sistema de origem                      ===> {source_system_path}\")\n",
    "  print(f\"Caminho para gravação do histórico     ===> {historic_path}\")\n",
    "except Exception as e:\n",
    "  print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "44fc1e2c-d501-4c64-bb68-9d0cc3792705",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Listando arquivos camada RAW"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "996bf487-9b3d-456a-9b1a-5f00585086dd",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Separando Arquivos de 10 em 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a8e628d7-d776-49bd-9779-08436a5e6968",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Criando Classe de Ingestao na Bronze"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4daf0652-140a-45df-98ad-9d2d7681f688",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "class BronzeIngestion:\n",
    "\n",
    "  def __init__(self,db,table,bronze_delta_path,source_system_path,historic_path):\n",
    "    self.db                 = db\n",
    "    self.table              = table\n",
    "    self.delta_table        = f\"{self.db}.{self.table}\"\n",
    "    self.bronze_delta_path  = bronze_delta_path\n",
    "    self.source_system_path = source_system_path\n",
    "    self.historic_path      = historic_path\n",
    "\n",
    "  ### Listando arquivos camada RAW   ####\n",
    "  def list_all_files(self):\n",
    "    try:\n",
    "      #### Faz a comparação entre Raw e histórico caso ja existam arquivos no Historico  ####\n",
    "      ################################################################################\n",
    "      \n",
    "      #### Lista de arquivos da Raw ####\n",
    "      lst_folder_date = [lst_date.name.replace('/','') for lst_date in dbutils.fs.ls(f'{self.source_system_path}') if lst_date.name.replace('/','').isnumeric() ]\n",
    "      lst_folder_date.sort()\n",
    "      raw_set = {files.path.split('/')[-2] for files in dbutils.fs.ls(f'{self.source_system_path}{lst_folder_date[-1]}/')} \n",
    "      ### Lista de arquivos Historico   ####\n",
    "      lst_folder_date_historic = [lst_date.name.replace('/','') for lst_date in dbutils.fs.ls(f'{self.historic_path}') if lst_date.name.replace('/','').isnumeric()  ]\n",
    "      lst_folder_date_historic.sort()\n",
    "      historic_set = {files.path.split('/')[-2]  for files in dbutils.fs.ls(f'{self.historic_path}{lst_folder_date_historic[-1]}/')}\n",
    "\n",
    "      all_files = [f'{self.source_system_path}{lst_folder_date[-1]}/{files}'  for files in list(raw_set.symmetric_difference(historic_set))]\n",
    "      print(f'Número de arquivos para ingestão == > {len(all_files)}')\n",
    "      return all_files\n",
    "\n",
    "    except:\n",
    "      try:\n",
    "        #### Caso naõa existm arquivos no histórico é listado todos arquivos na Raw ####\n",
    "        ################################################################################\n",
    "        ## Verifica a pasta data mais recente ###\n",
    "        lst_folder_date = [lst_date.name.replace('/','') for lst_date in dbutils.fs.ls(f'{self.source_system_path}')]\n",
    "        lst_folder_date.sort()\n",
    "        ### self.source_system_path => sistema de origem   ####\n",
    "        all_files = []\n",
    "        for files in dbutils.fs.ls(f'{self.source_system_path}{lst_folder_date[-1]}/'):\n",
    "          all_files.append([files.path,files.path.split(\"_\")[-1].replace(\"/\",\"\")])\n",
    "        all_files.sort(key = lambda pos:pos[1])\n",
    "        # print(all_files)\n",
    "        all_files_found = [all_files[i][0] for i in range(0,len(all_files))]\n",
    "        print(f'Número de arquivos para ingestão == > {len(all_files_found)}')\n",
    "        return all_files_found\n",
    "      except Exception as error:\n",
    "        print(f\"{error}\")       \n",
    "\n",
    "\n",
    "  \n",
    "  def separate_files(self):\n",
    "    try:\n",
    "      ### Separando arquivos a cada 10 items ou a quantidade que for..\n",
    "      ### caso seja menor do que 10\n",
    "      all_files = self.list_all_files()\n",
    "      return  [ all_files[i:i+10] for i in range(0,len(all_files),10)]\n",
    "    \n",
    "    except Exception as error:\n",
    "      print(f\"{error}\") \n",
    "  \n",
    "  ### Salvando Dados na camada Bronze ###\n",
    "  def save_files_delta(self):\n",
    "    try:\n",
    "      print(\" Inicinado método list_all_files().... \\n Inicinado método list_all_files() .... \\n Inicinado método save_files_delta() ....\\n\")\n",
    "      files_to_record = self.separate_files()\n",
    "      for files in files_to_record:\n",
    "        # print(files,len(files),\"\\n\")\n",
    "        df =spark.read.json(files)\n",
    "        #### Adicionando data de ingestao ####\n",
    "        df = df.withColumn('DTPROC',lit(datetime.now(pytz.timezone('America/Sao_Paulo')).strftime('%Y%m%d_%H%M%S')))\n",
    "        print(f\" Salvando dados da API no caminho ===> {self.bronze_delta_path}  com o nome ===>  {self.delta_table} \")\n",
    "        df.write.mode('append').format('delta').save(self.bronze_delta_path)\n",
    "      print(\"\\n Dados salvos em formato delta. \\n\")\n",
    "    except Exception as error:\n",
    "      print(f\"{error}\")\n",
    "\n",
    "  ##### Copiando arquivos para Historico  ####\n",
    "  def copy_to_historic(self):\n",
    "    try:\n",
    "      ## Verifica a pasta data mais recente ###\n",
    "      lst_folder_date = [lst_date.name.replace('/','') for lst_date in dbutils.fs.ls(f'{self.source_system_path}')]\n",
    "      lst_folder_date.sort()\n",
    "      print(f\"\\n Copiando arquivos de ==> {self.source_system_path}{lst_folder_date[-1]}/ para ==>  {self.historic_path}{lst_folder_date[-1]}/ \\n \")\n",
    "      dbutils.fs.cp(f'{self.source_system_path}{lst_folder_date[-1]}/',f'{self.historic_path}{lst_folder_date[-1]}/',recurse = True)\n",
    "      print(\" Copia realizada com sucesso.\\n\")\n",
    "    except Exception as error:\n",
    "      print(f\"{error}\")\n",
    "  \n",
    "  \n",
    "  ### Criando tabela no hive_metastore (catalogo Databricks)  ####\n",
    "  def create_delta_table_hive(self):\n",
    "    try:\n",
    "      sql = f\"\"\" CREATE DATABASE IF NOT EXISTS {self.db} \"\"\"\n",
    "      spark.sql(sql)\n",
    "      print(sql,\"\\n\")\n",
    "\n",
    "      sql_drop = f\"\"\" DROP TABLE IF EXISTS {self.delta_table}  \"\"\"\n",
    "      spark.sql(sql_drop)\n",
    "      print(sql_drop,\"\\n\") \n",
    "\n",
    "      sql_table = f\"\"\" CREATE TABLE IF NOT EXISTS {self.delta_table} USING DELTA LOCATION '{self.bronze_delta_path}' \"\"\"\n",
    "      spark.sql(sql_table)\n",
    "      print(sql_table)\n",
    "\n",
    "    except Exception as error:\n",
    "      print(f\"{error}\")\n",
    "\n",
    "  \n",
    "\n",
    "  #### Executando  ####\n",
    "  def bronze_run(self):\n",
    "    print(\"Inicio do processo de ingestão na bronze... \\n\")\n",
    "    # self.list_all_files()\n",
    "    # self.separate_files()\n",
    "    self.save_files_delta()\n",
    "    self.copy_to_historic()\n",
    "    self.create_delta_table_hive()\n",
    "    print(\"Processo finalizado!..\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "13567599-deff-42f7-bd9d-5f4153be8939",
     "showTitle": false,
     "title": ""
    }
   },
   "source": [
    "### Ajustar Copy to historic.. para copiar apenas lista de arquivos vrificadas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a2de4407-c507-469b-9d74-57219db9c223",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inicio do processo de ingestão na bronze... \n\n Inicinado método list_all_files().... \n Inicinado método list_all_files() .... \n Inicinado método save_files_delta() ....\n\nNúmero de arquivos para ingestão == > 1\n Salvando dados da API no caminho ===> dbfs:/mnt/bronze/  com o nome ===>  bronze.ibge_news \n\n Dados salvos em formato delta. \n\n\n Copiando arquivos de ==> dbfs:/mnt/raw_3/202410/ para ==>  dbfs:/mnt/historic/bronze/202410/ \n \n Copia realizada com sucesso.\n\n CREATE DATABASE IF NOT EXISTS bronze  \n\n DROP TABLE IF EXISTS bronze.ibge_news   \n\n CREATE TABLE IF NOT EXISTS bronze.ibge_news USING DELTA LOCATION 'dbfs:/mnt/bronze/' \nProcesso finalizado!..\n"
     ]
    }
   ],
   "source": [
    "bronze = BronzeIngestion('bronze','ibge_news','dbfs:/mnt/bronze/','dbfs:/mnt/raw_3/','dbfs:/mnt/historic/bronze/')\n",
    "bronze.bronze_run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ae74177f-ab33-4298-be5a-7f1c98333ad6",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/html": [
       "<style scoped>\n",
       "  .table-result-container {\n",
       "    max-height: 300px;\n",
       "    overflow: auto;\n",
       "  }\n",
       "  table, th, td {\n",
       "    border: 1px solid black;\n",
       "    border-collapse: collapse;\n",
       "  }\n",
       "  th, td {\n",
       "    padding: 5px;\n",
       "  }\n",
       "  th {\n",
       "    text-align: left;\n",
       "  }\n",
       "</style><div class='table-result-container'><table class='table-result'><thead style='background-color: white'><tr><th>count(1)</th></tr></thead><tbody><tr><td>30954</td></tr></tbody></table></div>"
      ]
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "addedWidgets": {},
       "aggData": [],
       "aggError": "",
       "aggOverflow": false,
       "aggSchema": [],
       "aggSeriesLimitReached": false,
       "aggType": "",
       "arguments": {},
       "columnCustomDisplayInfos": {},
       "data": [
        [
         30954
        ]
       ],
       "datasetInfos": [],
       "dbfsResultPath": null,
       "isJsonSchema": true,
       "metadata": {
        "dataframeName": "_sqldf",
        "executionCount": 96
       },
       "overflow": false,
       "plotOptions": {
        "customPlotOptions": {},
        "displayType": "table",
        "pivotAggregation": null,
        "pivotColumns": null,
        "xColumns": null,
        "yColumns": null
       },
       "removedWidgets": [],
       "schema": [
        {
         "metadata": "{\"__autoGeneratedAlias\":\"true\"}",
         "name": "count(1)",
         "type": "\"long\""
        }
       ],
       "type": "table"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%sql\n",
    "select count(*) from delta.`dbfs:/mnt/bronze/`\n",
    "-- select * from bronze.ibge_news"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a128b90a-bb20-4cba-a687-6b6ef76565ad",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# display(dbutils.fs.ls('dbfs:/mnt/historic/bronze/202410/'))\n",
    "\n",
    "## apagar pastas\n",
    "# dbutils.fs.rm('dbfs:/mnt/raw/',True)\n",
    "# dbutils.fs.rm('dbfs:/mnt/bronze/',True)\n",
    "# dbutils.fs.rm('dbfs:/mnt/historic/bronze/',True)\n",
    "\n",
    "## criar pasta\n",
    "# dbutils.fs.mkdirs('dbfs:/mnt/raw/')\n",
    "# dbutils.fs.mkdirs('dbfs:/mnt/raw_2/')\n",
    "# dbutils.fs.mkdirs('dbfs:/mnt/raw_3/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "465a52b2-97ed-45ef-9e8c-fcbd3e24c6b8",
     "showTitle": true,
     "title": "Leitura de arquivos de forma Multipla"
    }
   },
   "outputs": [],
   "source": [
    "# ### é Possivel fazer a Leitura de varios arquivos de forma Multipla, passando a referencia deles dentro de uma lista  ##########\n",
    "\n",
    "# spark.read.json(files).display()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "environmentMetadata": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1746308985242144,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "bronze_ingestion_run- camada BRONZE",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
